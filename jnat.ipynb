{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "VALID_CHAR_REGEX = \"[A-Z0-9]\"\n",
    "VALID_CHAR_CZECH_REGEX = \"[a-zA-ZáčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ]\" # czech lang\n",
    "VALID_CHAR_POLISH_REGEX = \"[a-zA-ZąćęłńóśżźĄĆĘŁŃÓŚŻŹ]\" # polish lang\n",
    "VALID_CHAR_GERMAN_REGEX = \"[a-zA-ZÄäÖöÜüẞß]\" # german lang\n",
    "VALID_CHAR_ENGLISH_REGEX = \"[a-zA-Z]\"\n",
    "INVALID_CHAR_REGEX = \"[.,()«»?!-—:;…]\"\n",
    "WOJNICZ_INPUT_FILE = \"inputs\\\\wojnicz.txt\"\n",
    "REAL_INPUT_FILE = \"inputs\\\\dinosauri-clean.txt\"\n",
    "DPI = 1\n",
    "FIGSIZE = (1200/DPI, 400/DPI)\n",
    "\n",
    "VALID_CHAR_REAL_REGEX = VALID_CHAR_CZECH_REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: save list of strings to file, separated by newlines\n",
    "def save_to_file(input: List[str], path: str):\n",
    "    file = open(path, \"w\")\n",
    "    for line in input[:-1]:\n",
    "        file.write(line + \"\\n\")\n",
    "    file.write(input[-1])\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_occ(input: dict[str, int], filename: str):\n",
    "    # generate occurance count graph\n",
    "    plt.plot(range(500), list(input.values())[:500], 'o', color='#444499')\n",
    "    plt.plot(range(200), list(input.values())[:200], 'o', color='#8888ff')\n",
    "\n",
    "    # save plot to file and show\n",
    "    if filename == \"\":\n",
    "        filename = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.savefig(\"outputs\\\\occurance_\" + filename + \".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: plot zipf numbers\n",
    "def plot_zipf(input: dict[str, float], filename: str):\n",
    "    sum = np.sum(list(input.values()))\n",
    "    avg = np.average(list(input.values())[:200])\n",
    "    avg_mid = np.average(list(input.values())[50:200])\n",
    "    first = 0\n",
    "    for z in input.values():\n",
    "        first += 1\n",
    "        if z >= avg:\n",
    "            break\n",
    "\n",
    "    print(\"Average value of first 200 items (highlighted):  \", avg)\n",
    "    print(\"Average value of 50th to 200th item:             \", avg_mid)\n",
    "    print(\"Index of first item to reach the average:        \", first)\n",
    "    \n",
    "    plt.plot(list(input.values())[:500], \"#444499\", label=\"Zipf values\")\n",
    "    plt.plot(list(input.values())[:200], \"#8888ff\", label=\"Zipf values (first 200)\")\n",
    "    plt.plot([avg for i in range(500)], \"#ff1155\", label=\"Average of first 200 Zipf values\")\n",
    "    plt.plot([avg_mid for i in range(500)], \"#ff9944\", label=\"Average of 50th to 200th Zipf values\")\n",
    "    plt.legend()\n",
    "\n",
    "    if filename == \"\":\n",
    "        filename = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.savefig(\"outputs\\\\zipf_\" + filename + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text file\n",
    "\n",
    "Removing all unnecesary lines and characters from input file.\n",
    "\n",
    "The end result is a text file with only lines containing words and no dashes at the end of the line.  \n",
    "Each line is separated by newline '<code>\\n</code>'  \n",
    "Each word within line is separated by comma '<code>,</code>'  \n",
    "  \n",
    "Output is saved to <code>cleaned.txt</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open input text\n",
    "# https://www.ic.unicamp.br/~stolfi/voynich/mirror/reeds/docs/FSG.txt\n",
    "text = open(WOJNICZ_INPUT_FILE, 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lines with no text\n",
    "parsed = []\n",
    "\n",
    "for line in text:\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    if line == \"\\x0c\":\n",
    "        continue\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "    if not re.search(VALID_CHAR_REGEX, line):\n",
    "        continue\n",
    "\n",
    "    parsed.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dashes and equality signs from end of each line\n",
    "cleaned = []\n",
    "\n",
    "for line in parsed:\n",
    "    # last valid char position\n",
    "    endpos = 0\n",
    "    for i, char in enumerate(line):\n",
    "        if re.match(VALID_CHAR_REGEX, char):\n",
    "            endpos = i\n",
    "\n",
    "    cleaned.append(line[:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "save_to_file(cleaned, \"cleaned.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting valid words\n",
    "This step further facilitates the analysis of the text.\n",
    "\n",
    "Separate words are now extracted to a single list of words.  \n",
    "Some words have not been transcripted fully and some characters may not be identified.  \n",
    "Since it is not definite what the words may actually be, they are going to be ommited. \n",
    "  \n",
    "Output is saved to <code>words.txt</code>, one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract valid words from each line\n",
    "words = []\n",
    "\n",
    "for line in cleaned:\n",
    "    tokens = line.split(\",\")\n",
    "    for word in tokens:\n",
    "        if re.match(\"^\"+VALID_CHAR_REGEX+\"*$\", word):\n",
    "            words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "save_to_file(words, \"words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing words\n",
    "\n",
    "Valid words are being mapped their count of occurance within the text.  \n",
    "\n",
    "Based on this data, the Zipf Law is applied to check if the text is written in a realistic human language.  \n",
    "Further analysis includes graphing and visualising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: map occurance count for each word and sort by occurance count descending\n",
    "def occurance_dict(input: List[str]):\n",
    "    output = {}\n",
    "\n",
    "    for word in input:\n",
    "        if word in output.keys():\n",
    "            output[word] += 1\n",
    "        else:\n",
    "            output[word] = 1\n",
    "            \n",
    "    output_desc = dict(sorted(output.items(), key=lambda item: item[1], reverse=True))\n",
    "    return output_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: get occurance percentages for each word\n",
    "def occurance_percentage(input: List[str]):\n",
    "    word_count = len(input)\n",
    "    count_dict = occurance_dict(input)\n",
    "    output = {}\n",
    "\n",
    "    for word in count_dict.keys():\n",
    "        output[word] = count_dict[word] / word_count\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: calculate zipf value for each word (occurance percentage * index) \n",
    "def zipf_values(input: List[str]):\n",
    "    word_count = len(input)\n",
    "    perc_dict = occurance_percentage(input)\n",
    "    output = {}\n",
    "\n",
    "    for i, word in enumerate(perc_dict.keys()):\n",
    "        output[word] = perc_dict[word] * (i+1) * 100\n",
    "\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all statistics for the text\n",
    "wojnicz_count = len(words)\n",
    "wojnicz_occ = occurance_dict(words)\n",
    "wojnicz_perc = occurance_percentage(words)\n",
    "wojnicz_zipf = zipf_values(words)\n",
    "wojnicz_zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of all valid words found in text:         \", wojnicz_count)\n",
    "plot_zipf(wojnicz_zipf, \"wojnicz\")\n",
    "plot_occ(wojnicz_occ, \"wojnicz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting and analyzing real-world language\n",
    "\n",
    "Text content of a selected source is converted to simillar format as the source text in order to analyze it in the same way.  \n",
    "After that, it is used to calculate the same statistics as with the previous text.\n",
    "\n",
    "Converted text is saved to <code>words_real.txt</code>, one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text from file\n",
    "text = open(REAL_INPUT_FILE, 'r', encoding=\"utf8\").read().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to list of words\n",
    "parsed = \"\"\n",
    "\n",
    "for char in text:\n",
    "    if char == ' ' or re.match(VALID_CHAR_REAL_REGEX, char):\n",
    "        parsed += char\n",
    "\n",
    "parsed = parsed.split(\" \")\n",
    "words = []\n",
    "\n",
    "for i, word in enumerate(parsed):\n",
    "    if re.match(\"^\"+VALID_CHAR_REAL_REGEX+\"+$\", word):\n",
    "        words.append(word.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "save_to_file(words, \"words_real.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze text\n",
    "real_count = len(words)\n",
    "real_occ = occurance_dict(words)\n",
    "real_perc = occurance_percentage(words)\n",
    "real_zipf = zipf_values(words)\n",
    "real_zipf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = re.sub(\"^inputs\\\\\\\\\", \"\", re.sub(\"\\\\.txt$\", \"\", REAL_INPUT_FILE))\n",
    "\n",
    "print(\"Number of all valid words found in text:         \", real_count)\n",
    "plot_zipf(real_zipf, graph_name)\n",
    "plot_occ(real_occ, graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common analysis\n",
    "\n",
    "Statistics are analyzed together for comparison.  \n",
    "In addition, real-world language source is trimmed to contain the exact same amount of words as the Wojnicz manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze portion of real text of same length as wojnicz manuscrypt\n",
    "rshort_occ = occurance_dict(words[:wojnicz_count])\n",
    "rshort_perc = occurance_percentage(words[:wojnicz_count])\n",
    "rshort_zipf = zipf_values(words[:wojnicz_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(wojnicz_zipf.values())[:100], \"#0000ff\", label=\"Wojnicz language\")\n",
    "plt.plot(list(real_zipf.values())[:100], \"#ee6666\", label=\"Real language - full sample\")\n",
    "plt.plot(list(rshort_zipf.values())[:100], \"#ff0000\", label=\"Real language - limited sample\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"outputs\\\\comparison_\" + graph_name + \".png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5cb8aafaeb8e0a18ee17c25f5e27cfa4722b633ed7a736629248d19340d2b0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text file\n",
    "\n",
    "Removing all unnecesary lines and characters from input file (<code>FSG.txt</code>).\n",
    "\n",
    "The end result is a text file with only lines containing words and no dashes at the end of the line.  \n",
    "Each line is separated by newline '<code>\\n</code>'  \n",
    "Each word within line is separated by comma '<code>,</code>'  \n",
    "  \n",
    "Output is saved to <code>cleaned.txt</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "VALID_CHAR_REGEX = \"[A-Z0-9]\"\n",
    "VALID_CHAR_REAL_REGEX = \"[a-zA-ZáčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ]\" # czech lang\n",
    "INVALID_CHAR_REGEX = \"[.,()-]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open input text\n",
    "# https://www.ic.unicamp.br/~stolfi/voynich/mirror/reeds/docs/FSG.txt\n",
    "text = open('FSG.txt', 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lines with no text\n",
    "parsed = []\n",
    "\n",
    "for line in text:\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    if line == \"\\x0c\":\n",
    "        continue\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "    if not re.search(VALID_CHAR_REGEX, line):\n",
    "        continue\n",
    "\n",
    "    parsed.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dashes and equality signs from end of each line\n",
    "cleaned = []\n",
    "\n",
    "for line in parsed:\n",
    "    # last valid char position\n",
    "    endpos = 0\n",
    "    for i, char in enumerate(line):\n",
    "        if re.match(VALID_CHAR_REGEX, char):\n",
    "            endpos = i\n",
    "\n",
    "    cleaned.append(line[:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "file = open(\"cleaned.txt\", \"w\")\n",
    "for line in cleaned[:-1]:\n",
    "    file.write(line + \"\\n\")\n",
    "file.write(cleaned[-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting valid words\n",
    "This step further facilitates the analysis of the text.\n",
    "\n",
    "Separate words are now extracted to a single list of words.  \n",
    "Some words have not been transcripted fully and some characters may not be identified.  \n",
    "Since it is not definite what the words may actually be, they are going to be ommited. \n",
    "  \n",
    "Output is saved to <code>words.txt</code>, one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open input file\n",
    "text = open('cleaned.txt', 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract valid words from each line\n",
    "words = []\n",
    "\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    list = line.split(\",\")\n",
    "    for word in list:\n",
    "        if re.match(\"^\"+VALID_CHAR_REGEX+\"*$\", word):\n",
    "            words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "file = open(\"words.txt\", \"w\")\n",
    "for line in words[:-1]:\n",
    "    file.write(line + \"\\n\")\n",
    "file.write(words[-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing words\n",
    "\n",
    "Valid words are being mapped their count of occurance within the text.  \n",
    "\n",
    "Based on this data, the Zipf Law is applied to check if the text is written in a realistic human language.  \n",
    "Further analysis includes graphing and visualising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function: map occurance count for each word and sort by occurance count descending\n",
    "def occurance_dict(input: List[str]):\n",
    "    output = {}\n",
    "\n",
    "    for word in input:\n",
    "        if word in output.keys():\n",
    "            output[word] += 1\n",
    "        else:\n",
    "            output[word] = 1\n",
    "            \n",
    "    output_desc = dict(sorted(output.items(), key=lambda item: item[1], reverse=True))\n",
    "    return output_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occurance percentages and calculate zipf value for each word (occurance percentage * index) \n",
    "def zipf_values(input: List[str]):\n",
    "    word_count = len(input)\n",
    "    count_dict = occurance_dict(input)\n",
    "    occurance_percentage = {}\n",
    "\n",
    "    for word in count_dict.keys():\n",
    "        occurance_percentage[word] = count_dict[word] / word_count\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    for i, word in enumerate(occurance_percentage.keys()):\n",
    "        output[word] = occurance_percentage[word] * (i+1) * 100\n",
    "\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open input file\n",
    "text = open('words.txt', 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map occurance count for each word\n",
    "wojnicz_dict_desc = occurance_dict(text)\n",
    "wojnicz_dict_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate zipf value for each word (occurance percentage * index) \n",
    "zipf = zipf_values(text)\n",
    "zipf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting and analyzing Wikipedia page\n",
    "\n",
    "Text content of a Wikipedia page is converted to simillar format as the source text in order to analyze it in the same way.  \n",
    "After that, it is used to calculate the same statistics as with the previous text.\n",
    "\n",
    "Converted text is saved to <code>words_wiki.txt</code>, one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text from file\n",
    "text = open('kralovec.txt', 'r', encoding=\"utf8\").read().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to list of words\n",
    "parsed = \"\"\n",
    "\n",
    "for char in text:\n",
    "    if char == ' ' or re.match(VALID_CHAR_REAL_REGEX, char):\n",
    "        parsed += char\n",
    "\n",
    "parsed = parsed.split(\" \")\n",
    "words = []\n",
    "\n",
    "for i, word in enumerate(parsed):\n",
    "    if re.match(\"^\"+VALID_CHAR_REAL_REGEX+\"+$\", word):\n",
    "        words.append(word.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "file = open(\"words_wiki.txt\", \"w\")\n",
    "for line in words[:-1]:\n",
    "    file.write(line + \"\\n\")\n",
    "file.write(words[-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze text\n",
    "zipf_wiki = zipf_values(words)\n",
    "zipf_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- sprawdzić czy to wgl to o co chodzi bo robiłem z pamięci\n",
    "- jakieś wykresy gośc chciał\n",
    "- potem jeszce jakiś graf dwudzielny (????????????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5cb8aafaeb8e0a18ee17c25f5e27cfa4722b633ed7a736629248d19340d2b0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
